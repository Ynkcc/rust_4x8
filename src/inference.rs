// inference.rs - æ¨ç†æœåŠ¡å™¨æ¨¡å—
//
// æä¾›æ‰¹é‡ç¥ç»ç½‘ç»œæ¨ç†æœåŠ¡ï¼Œç”¨äºæ”¯æŒå¹¶è¡Œè‡ªå¯¹å¼ˆè®­ç»ƒ
// é€šè¿‡é€šé“æ”¶é›†å¤šä¸ªæ¨ç†è¯·æ±‚ï¼Œæ‰¹é‡å¤„ç†ä»¥æé«˜GPUåˆ©ç”¨ç‡

use crate::game_env::{
    DarkChessEnv, Observation, ACTION_SPACE_SIZE, BOARD_CHANNELS, BOARD_COLS, BOARD_ROWS,
    SCALAR_FEATURE_COUNT,
};
use crate::mcts::Evaluator;
use crate::nn_model::BanqiNet;
use anyhow::Result;
use std::sync::mpsc;
use std::time::Duration;
use tch::{nn, Device, Kind, Tensor};

// ================ æ¨ç†è¯·æ±‚å’Œå“åº” ================

/// æ¨ç†è¯·æ±‚
#[derive(Debug)]
pub struct InferenceRequest {
    pub observation: Observation,
    pub action_masks: Vec<i32>,
    pub response_tx: mpsc::Sender<InferenceResponse>, // æ¯ä¸ªè¯·æ±‚æºå¸¦è‡ªå·±çš„å“åº”é€šé“
}

/// æ¨ç†å“åº”
#[derive(Debug, Clone)]
pub struct InferenceResponse {
    pub policy: Vec<f32>,
    pub value: f32,
}

// ================ æ‰¹é‡æ¨ç†æœåŠ¡å™¨ ================

pub struct InferenceServer {
    _vs: nn::VarStore, // æŒæœ‰ VarStoreï¼ˆåŒ…å«æ¨¡å‹æƒé‡ï¼‰- åŠ ä¸‹åˆ’çº¿é¿å…è­¦å‘Š
    net: BanqiNet,     // ç½‘ç»œç»“æ„
    device: Device,
    request_rx: mpsc::Receiver<InferenceRequest>,
    batch_size: usize,
    batch_timeout_ms: u64,
}

impl InferenceServer {
    pub fn new(
        model_path: &str,
        device: Device,
        request_rx: mpsc::Receiver<InferenceRequest>,
        batch_size: usize,
        batch_timeout_ms: u64,
    ) -> Result<Self> {
        let mut vs = nn::VarStore::new(device);
        let net = BanqiNet::new(&vs.root());

        // åŠ è½½æ¨¡å‹æƒé‡
        vs.load(model_path)?;

        Ok(Self {
            _vs: vs,
            net,
            device,
            request_rx,
            batch_size,
            batch_timeout_ms,
        })
    }

    /// è¿è¡Œæ¨ç†æœåŠ¡ï¼ˆé˜»å¡ï¼‰
    pub fn run(&self) {
        println!(
            "[InferenceServer] å¯åŠ¨ï¼Œbatch_size={}, timeout={}ms",
            self.batch_size, self.batch_timeout_ms
        );

        let mut batch = Vec::new();
        let mut total_requests = 0;
        let mut total_batches = 0;
        let batch_timeout = Duration::from_millis(self.batch_timeout_ms);

        loop {
            // å°è¯•å¿«é€Ÿæ”¶é›†ä¸€æ‰¹è¯·æ±‚

            // é¦–å…ˆå°è¯•éé˜»å¡æ¥æ”¶ï¼Œå¿«é€Ÿæ”¶é›†å¯ç”¨çš„è¯·æ±‚
            loop {
                match self.request_rx.try_recv() {
                    Ok(req) => {
                        batch.push(req);
                        total_requests += 1;

                        // å¦‚æœè¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œç«‹å³å¤„ç†
                        if batch.len() >= self.batch_size {
                            break;
                        }
                    }
                    Err(mpsc::TryRecvError::Empty) => {
                        // æ²¡æœ‰æ›´å¤šè¯·æ±‚äº†
                        break;
                    }
                    Err(mpsc::TryRecvError::Disconnected) => {
                        // æ‰€æœ‰å‘é€è€…å·²æ–­å¼€
                        if !batch.is_empty() {
                            println!("[InferenceServer] æœ€ç»ˆæ‰¹æ¬¡: {} ä¸ªè¯·æ±‚", batch.len());
                            self.process_batch(&batch);
                            total_batches += 1;
                        }
                        println!(
                            "[InferenceServer] æ‰€æœ‰å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œé€€å‡º (æ€»è®¡: {} è¯·æ±‚, {} æ‰¹æ¬¡)",
                            total_requests, total_batches
                        );
                        return;
                    }
                }
            }

            // å¦‚æœæ”¶é›†åˆ°äº†è¯·æ±‚ï¼Œç«‹å³å¤„ç†ï¼ˆä¸ç­‰å¾…è¶…æ—¶ï¼‰
            if !batch.is_empty() {
                if total_batches % 4000 == 0 {
                    println!(
                        "[InferenceServer] å¤„ç†æ‰¹æ¬¡#{}: {} ä¸ªè¯·æ±‚",
                        total_batches + 1,
                        batch.len()
                    );
                }
                self.process_batch(&batch);
                total_batches += 1;
                batch.clear();
                continue;
            }

            // å¦‚æœæ²¡æœ‰è¯·æ±‚ï¼Œé˜»å¡ç­‰å¾…æ–°è¯·æ±‚ï¼ˆå¸¦è¶…æ—¶ï¼‰
            match self.request_rx.recv_timeout(batch_timeout) {
                Ok(req) => {
                    batch.push(req);
                    total_requests += 1;
                }
                Err(mpsc::RecvTimeoutError::Timeout) => {
                    // è¶…æ—¶ä½†æ²¡æœ‰è¯·æ±‚ï¼Œç»§ç»­ç­‰å¾…
                    continue;
                }
                Err(mpsc::RecvTimeoutError::Disconnected) => {
                    println!(
                        "[InferenceServer] æ‰€æœ‰å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œé€€å‡º (æ€»è®¡: {} è¯·æ±‚, {} æ‰¹æ¬¡)",
                        total_requests, total_batches
                    );
                    return;
                }
            }
        }
    }

    /// æ‰¹é‡å¤„ç†æ¨ç†è¯·æ±‚
    fn process_batch(&self, batch: &Vec<InferenceRequest>) {
        if batch.is_empty() {
            return;
        }

        // let start_time = Instant::now();
        let batch_len = batch.len();

        // ğŸ”¥ å…³é”®ä¿®å¤ï¼šåœ¨ no_grad ä¸­æ‰§è¡Œæ•´ä¸ªæ¨ç†æµç¨‹ï¼Œç¡®ä¿ä¸æ„å»ºæ¢¯åº¦å›¾
        tch::no_grad(|| {
            // å‡†å¤‡æ‰¹é‡è¾“å…¥å¼ é‡
            let mut board_data = Vec::new();
            let mut scalar_data = Vec::new();
            let mut mask_data = Vec::new();

            for req in batch {
                // Board: [STATE_STACK_SIZE, BOARD_CHANNELS, BOARD_ROWS, BOARD_COLS] -> flatten
                let board_flat: Vec<f32> = req.observation.board.as_slice().unwrap().to_vec();
                board_data.extend_from_slice(&board_flat);

                // Scalars: [STATE_STACK_SIZE * SCALAR_FEATURE_COUNT]
                let scalars_flat: Vec<f32> = req.observation.scalars.as_slice().unwrap().to_vec();
                scalar_data.extend_from_slice(&scalars_flat);

                // Masks: [ACTION_SPACE_SIZE]
                let masks_f32: Vec<f32> = req.action_masks.iter().map(|&m| m as f32).collect();
                mask_data.extend_from_slice(&masks_f32);
            }

            // æ„å»ºå¼ é‡: [batch, C, H, W]
            let board_tensor = Tensor::from_slice(&board_data)
                .view([
                    batch_len as i64,
                    BOARD_CHANNELS as i64,
                    BOARD_ROWS as i64,
                    BOARD_COLS as i64,
                ])
                .to(self.device);

            let scalar_tensor = Tensor::from_slice(&scalar_data)
                .view([batch_len as i64, SCALAR_FEATURE_COUNT as i64])
                .to(self.device);

            let mask_tensor = Tensor::from_slice(&mask_data)
                .view([batch_len as i64, ACTION_SPACE_SIZE as i64])
                .to(self.device);

            // å‰å‘æ¨ç†ï¼ˆå·²åœ¨ no_grad ä¸­ï¼Œæ— éœ€é‡å¤åŒ…è£¹ï¼‰
            let (logits, values) = self.net.forward_inference(&board_tensor, &scalar_tensor);

            // åº”ç”¨æ©ç å¹¶è®¡ç®—æ¦‚ç‡
            let masked_logits = &logits + (&mask_tensor - 1.0) * 1e9;
            let probs = masked_logits.softmax(-1, Kind::Float);

            // æå–ç»“æœå¹¶å‘é€å“åº”åˆ°å„è‡ªçš„é€šé“
            for (i, req) in batch.iter().enumerate() {
                let policy_slice = probs.get(i as i64);
                let mut policy = vec![0.0f32; ACTION_SPACE_SIZE];
                policy_slice
                    .to_device(Device::Cpu)
                    .copy_data(&mut policy, ACTION_SPACE_SIZE);

                let value = values.get(i as i64).squeeze().double_value(&[]) as f32;

                let response = InferenceResponse { policy, value };

                // å‘é€å“åº”åˆ°è¯·æ±‚è€…çš„ä¸“å±é€šé“ï¼ˆå¿½ç•¥å‘é€å¤±è´¥ï¼‰
                let _ = req.response_tx.send(response);
                
                // ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ˜¾å¼é‡Šæ”¾ä¸´æ—¶åˆ‡ç‰‡å¼ é‡
                drop(policy_slice);
            }
            
            // ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ˜¾å¼é‡Šæ”¾æ‰€æœ‰ä¸­é—´å¼ é‡
            drop(board_tensor);
            drop(scalar_tensor);
            drop(mask_tensor);
            drop(logits);
            drop(values);
            drop(masked_logits);
            drop(probs);
        });

        // let elapsed = start_time.elapsed();
        // if batch_len >= 4 {  // åªåœ¨æ‰¹é‡è¾ƒå¤§æ—¶è¾“å‡ºæ—¥å¿—
        //     println!("[InferenceServer] æ‰¹æ¬¡å¤„ç†: {} ä¸ªè¯·æ±‚è€—æ—¶ {:.2}ms",
        //         batch_len, elapsed.as_secs_f64() * 1000.0);
        // }
    }
}

// ================ Channel Evaluatorï¼ˆç”¨äºMCTSï¼‰ ================

pub struct ChannelEvaluator {
    request_tx: mpsc::Sender<InferenceRequest>,
}

impl ChannelEvaluator {
    pub fn new(request_tx: mpsc::Sender<InferenceRequest>) -> Self {
        Self { request_tx }
    }
}

impl Evaluator for ChannelEvaluator {
    fn evaluate(&self, env: &DarkChessEnv) -> (Vec<f32>, f32) {
        // ä¸ºæ­¤æ¬¡è¯·æ±‚åˆ›å»ºä¸€æ¬¡æ€§å“åº”é€šé“
        let (response_tx, response_rx) = mpsc::channel();

        // å‘é€æ¨ç†è¯·æ±‚
        let mut masks = vec![0; crate::game_env::ACTION_SPACE_SIZE];
        env.action_masks_into(&mut masks);
        let req = InferenceRequest {
            observation: env.get_state(),
            action_masks: masks,
            response_tx,
        };

        self.request_tx.send(req).expect("æ¨ç†æœåŠ¡å·²æ–­å¼€");

        // ç­‰å¾…å“åº”ï¼ˆé˜»å¡ï¼‰
        let resp = response_rx.recv().expect("æ¨ç†æœåŠ¡æ— å“åº”");

        (resp.policy, resp.value)
    }
}
