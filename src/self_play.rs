// self_play.rs - è‡ªå¯¹å¼ˆå·¥ä½œå™¨æ¨¡å—
//
// æä¾›è‡ªå¯¹å¼ˆæ¸¸æˆçš„æ‰§è¡Œé€»è¾‘ï¼ŒåŒ…æ‹¬å·¥ä½œå™¨ç®¡ç†ã€åŠ¨ä½œé‡‡æ ·ã€åœºæ™¯ç±»å‹ç­‰

use crate::game_env::{DarkChessEnv, Observation, Player};
use crate::inference::ChannelEvaluator;
use crate::mcts::{MCTSConfig, MCTS};
use rand::distributions::WeightedIndex;
use rand::prelude::*;
use std::sync::Arc;
use std::time::Instant;

// ================ æ¸¸æˆç»Ÿè®¡ä¿¡æ¯ ================

/// æ¸¸æˆç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct GameStats {
    pub steps: usize,
    pub winner: Option<i32>, // Some(1)=çº¢èƒœ, Some(-1)=é»‘èƒœ, None/Some(0)=å¹³å±€
}

/// å•å±€æ¸¸æˆçš„å®Œæ•´æ•°æ®ï¼ˆåŒ…å«æ ·æœ¬å’Œå…ƒæ•°æ®ï¼‰
#[derive(Debug, Clone)]
pub struct GameEpisode {
    pub samples: Vec<(Observation, Vec<f32>, f32, f32, Vec<i32>)>, // (è§‚å¯Ÿ, ç­–ç•¥æ¦‚ç‡, MCTSä»·å€¼, æ¸¸æˆç»“æœä»·å€¼, åŠ¨ä½œæ©ç )
    pub game_length: usize,
    pub winner: Option<i32>,
}

// ================ åœºæ™¯ç¯å¢ƒæšä¸¾ ================

/// åœºæ™¯ç±»å‹æšä¸¾ï¼Œç”¨äºæŒ‡å®šè‡ªå¯¹å¼ˆä½¿ç”¨çš„åœºæ™¯
#[derive(Debug, Clone, Copy)]
pub enum ScenarioType {
    /// åœºæ™¯1: R_A vs B_A (çº¢ä»•å¯¹é»‘ä»•)
    TwoAdvisors,
    /// åœºæ™¯2: Hidden Threat (éšè—å¨èƒ)
    HiddenThreats,
    /// æ ‡å‡†å¼€å±€
    Standard,
}

impl ScenarioType {
    /// åˆ›å»ºå¯¹åº”åœºæ™¯çš„ç¯å¢ƒ
    pub fn create_env(&self) -> DarkChessEnv {
        let mut env = DarkChessEnv::new();
        match self {
            ScenarioType::TwoAdvisors => env.setup_two_advisors(Player::Black),
            ScenarioType::HiddenThreats => env.setup_hidden_threats(),
            ScenarioType::Standard => {}
        }
        env
    }

    /// è·å–åœºæ™¯åç§°
    pub fn name(&self) -> &'static str {
        match self {
            ScenarioType::TwoAdvisors => "TwoAdvisors (R_A vs B_A)",
            ScenarioType::HiddenThreats => "HiddenThreats",
            ScenarioType::Standard => "Standard",
        }
    }

    /// è·å–è¯¥åœºæ™¯çš„æœŸæœ›æœ€ä¼˜åŠ¨ä½œ
    pub fn expected_action(&self) -> usize {
        match self {
            ScenarioType::TwoAdvisors => 38,
            ScenarioType::HiddenThreats => 3,
            ScenarioType::Standard => 0,
        }
    }
}

// ================ å¹¶è¡Œè‡ªå¯¹å¼ˆå·¥ä½œå™¨ ================

/// è‡ªå¯¹å¼ˆå·¥ä½œå™¨
pub struct SelfPlayWorker {
    pub worker_id: usize,
    pub evaluator: Arc<ChannelEvaluator>,
    pub mcts_sims: usize,
    pub scenario: Option<ScenarioType>, // æŒ‡å®šåœºæ™¯ç±»å‹ï¼ŒNone è¡¨ç¤ºä½¿ç”¨éšæœºåˆå§‹åŒ–
    pub dirichlet_alpha: f32,            // Dirichlet å™ªå£° alpha å‚æ•°
    pub dirichlet_epsilon: f32,          // Dirichlet å™ªå£°æƒé‡
}

impl SelfPlayWorker {
    pub fn new(worker_id: usize, evaluator: Arc<ChannelEvaluator>, mcts_sims: usize) -> Self {
        Self {
            worker_id,
            evaluator,
            mcts_sims,
            scenario: None,
            dirichlet_alpha: 0.3,
            dirichlet_epsilon: 0.25,
        }
    }

    /// åˆ›å»ºä½¿ç”¨æŒ‡å®šåœºæ™¯çš„å·¥ä½œå™¨
    pub fn with_scenario(
        worker_id: usize,
        evaluator: Arc<ChannelEvaluator>,
        mcts_sims: usize,
        scenario: ScenarioType,
    ) -> Self {
        Self {
            worker_id,
            evaluator,
            mcts_sims,
            scenario: Some(scenario),
            dirichlet_alpha: 0.3,
            dirichlet_epsilon: 0.25,
        }
    }
    
    /// åˆ›å»ºä½¿ç”¨æŒ‡å®šåœºæ™¯å’Œ Dirichlet å‚æ•°çš„å·¥ä½œå™¨
    pub fn with_scenario_and_dirichlet(
        worker_id: usize,
        evaluator: Arc<ChannelEvaluator>,
        mcts_sims: usize,
        scenario: ScenarioType,
        dirichlet_alpha: f32,
        dirichlet_epsilon: f32,
    ) -> Self {
        Self {
            worker_id,
            evaluator,
            mcts_sims,
            scenario: Some(scenario),
            dirichlet_alpha,
            dirichlet_epsilon,
        }
    }

    /// è¿è¡Œä¸€å±€è‡ªå¯¹å¼ˆæ¸¸æˆï¼Œè¿”å›GameEpisode
    pub fn play_episode(&self, episode_num: usize) -> GameEpisode {
        let _scenario_name = self.scenario.map(|s| s.name()).unwrap_or("Random");
        // println!("  [Worker-{}] å¼€å§‹ç¬¬ {} å±€æ¸¸æˆ (åœºæ™¯: {})", self.worker_id, episode_num + 1, _scenario_name);
        let start_time = Instant::now();

        // æ ¹æ®åœºæ™¯ç±»å‹åˆ›å»ºç¯å¢ƒ
        let mut env = match self.scenario {
            Some(scenario) => scenario.create_env(),
            None => DarkChessEnv::new(),
        };
        let config = MCTSConfig {
            num_simulations: self.mcts_sims,
            cpuct: 1.0,
            virtual_loss: 1.0,
            max_concurrent_inferences: 8,
            dirichlet_alpha: self.dirichlet_alpha,
            dirichlet_epsilon: self.dirichlet_epsilon,
            train: true, // è‡ªå¯¹å¼ˆè®­ç»ƒæ—¶å¼€å¯ Dirichlet å™ªå£°
        };
        let mut mcts = MCTS::new(&env, self.evaluator.clone(), config);

        let mut episode_data = Vec::new();
        let mut step = 0;

        // ğŸ› DEBUG: è®°å½•é¦–æ­¥MCTSè¯¦æƒ…
        let debug_first_step = episode_num < 2; // åªè°ƒè¯•å‰2å±€

        loop {
            // è¿è¡ŒMCTS
            mcts.run();
            let probs = mcts.get_root_probabilities();
            let masks = env.action_masks();
            
            // è·å–MCTSæ ¹èŠ‚ç‚¹çš„ä»·å€¼ï¼ˆä»å½“å‰ç©å®¶è§†è§’ï¼‰
            let mcts_value = mcts.root.q_value();

            // ğŸ› DEBUG: æ‰“å°MCTSæ ¹èŠ‚ç‚¹è¯¦æƒ…
            if debug_first_step && step < 3 {
                // println!("    [Worker-{}] Step {}: MCTSæ ¹èŠ‚ç‚¹è¯¦æƒ…", self.worker_id, step);
                let _top_actions = get_top_k_actions(&probs, 5);
                // for (_action, _prob) in _top_actions {
                //     println!("      action={}, prob={:.3}", _action, _prob);
                // }
            }

            // ä¿å­˜æ•°æ®ï¼ˆåŒ…å«MCTSä»·å€¼ï¼‰
            episode_data.push((
                env.get_state(),
                probs.clone(),
                mcts_value,
                env.get_current_player(),
                masks,
            ));

            // é€‰æ‹©åŠ¨ä½œï¼ˆä½¿ç”¨è®¿é—®è®¡æ•°æ¯”ä¾‹ï¼Œä¸å†ä½¿ç”¨æ¸©åº¦é‡‡æ ·ï¼‰
            // Dirichlet å™ªå£°å·²ç»åœ¨ MCTS æ ¹èŠ‚ç‚¹æ‰©å±•æ—¶æ·»åŠ 
            let action = sample_action(&probs, &env, 1.0);

            // ğŸ› DEBUG: è®°å½•åŠ¨ä½œé€‰æ‹©
            if debug_first_step && step < 3 {
                // println!("      é€‰æ‹©: action={}", action);
            }

            // æ‰§è¡ŒåŠ¨ä½œ
            match env.step(action, None) {
                Ok((_, _, terminated, truncated, winner)) => {
                    mcts.step_next(&env, action);

                    if terminated || truncated {
                        // åˆ†é…å¥–åŠ±
                        let reward_red = match winner {
                            Some(1) => 1.0,
                            Some(-1) => -1.0,
                            _ => 0.0,
                        };

                        let _elapsed = start_time.elapsed();
                        // println!("  [Worker-{}] ç¬¬ {} å±€ç»“æŸ: {} æ­¥, èƒœè€…={:?}, è€—æ—¶ {:.1}s",
                        //     self.worker_id, episode_num + 1, step, winner, _elapsed.as_secs_f64());

                        // ğŸ› DEBUG: æ£€æŸ¥ä»·å€¼æ ‡ç­¾åˆ†å¸ƒ
                        if debug_first_step {
                            let mut red_values = Vec::new();
                            let mut black_values = Vec::new();
                            for (_, _, _, player, _) in &episode_data {
                                let val = if player.val() == 1 {
                                    reward_red
                                } else {
                                    -reward_red
                                };
                                if player.val() == 1 {
                                    red_values.push(val);
                                } else {
                                    black_values.push(val);
                                }
                            }
                            // println!("    [Worker-{}] ä»·å€¼æ ‡ç­¾ç»Ÿè®¡: çº¢æ–¹æ ·æœ¬æ•°={}, é»‘æ–¹æ ·æœ¬æ•°={}",
                            //     self.worker_id, red_values.len(), black_values.len());
                            if !red_values.is_empty() {
                                // println!("      çº¢æ–¹ä»·å€¼æ ‡ç­¾: {:.2} (winner={:?})", red_values[0], winner);
                            }
                            if !black_values.is_empty() {
                                // println!("      é»‘æ–¹ä»·å€¼æ ‡ç­¾: {:.2} (winner={:?})", black_values[0], winner);
                            }
                        }

                        // å›å¡«ä»·å€¼
                        let mut samples = Vec::new();
                        for (obs, p, mcts_val, player, mask) in episode_data {
                            let game_result_val = if player.val() == 1 {
                                reward_red
                            } else {
                                -reward_red
                            };
                            samples.push((obs, p, mcts_val, game_result_val, mask));
                        }

                        return GameEpisode {
                            samples,
                            game_length: step,
                            winner,
                        };
                    }
                }
                Err(_e) => {
                    // eprintln!("[Worker-{}] æ¸¸æˆé”™è¯¯: {}", self.worker_id, _e);
                    return GameEpisode {
                        samples: Vec::new(),
                        game_length: step,
                        winner: None,
                    };
                }
            }

            step += 1;
            if step > 200 {
                // è¶…è¿‡æœ€å¤§æ­¥æ•°ï¼Œæ¸¸æˆå¹³å±€
                // println!("  [Worker-{}] ç¬¬ {} å±€è¶…æ—¶: {} æ­¥", self.worker_id, episode_num + 1, step);
                let mut samples = Vec::new();
                for (obs, p, mcts_val, _, mask) in episode_data {
                    samples.push((obs, p, mcts_val, 0.0, mask));
                }
                return GameEpisode {
                    samples,
                    game_length: step,
                    winner: None,
                };
            }
        }
    }
}

// ================ è¾…åŠ©å‡½æ•° ================

/// åŠ¨ä½œé‡‡æ ·ï¼ˆå¸¦æ¸©åº¦å‚æ•°ï¼‰
pub fn sample_action(probs: &[f32], env: &DarkChessEnv, temperature: f32) -> usize {
    let non_zero_sum: f32 = probs.iter().sum();

    if non_zero_sum == 0.0 {
        // å›é€€ï¼šä»æœ‰æ•ˆåŠ¨ä½œä¸­å‡åŒ€é€‰æ‹©
        let masks = env.action_masks();
        let valid_actions: Vec<usize> = masks
            .iter()
            .enumerate()
            .filter_map(|(i, &m)| if m == 1 { Some(i) } else { None })
            .collect();

        let mut rng = thread_rng();
        *valid_actions.choose(&mut rng).expect("æ— æœ‰æ•ˆåŠ¨ä½œ")
    } else {
        // åº”ç”¨æ¸©åº¦å‚æ•°
        let adjusted_probs: Vec<f32> = if temperature != 1.0 {
            let sum: f32 = probs.iter().map(|&p| p.powf(1.0 / temperature)).sum();
            probs
                .iter()
                .map(|&p| p.powf(1.0 / temperature) / sum)
                .collect()
        } else {
            probs.to_vec()
        };

        let dist = WeightedIndex::new(&adjusted_probs).unwrap();
        let mut rng = thread_rng();
        dist.sample(&mut rng)
    }
}

/// ğŸ› DEBUG: è·å–top-kåŠ¨ä½œ
pub fn get_top_k_actions(probs: &[f32], k: usize) -> Vec<(usize, f32)> {
    let mut indexed: Vec<(usize, f32)> = probs.iter().enumerate().map(|(i, &p)| (i, p)).collect();
    indexed.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
    indexed.into_iter().take(k).collect()
}
