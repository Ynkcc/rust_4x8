// parallel_train.rs - å¹¶è¡Œè‡ªå¯¹å¼ˆè®­ç»ƒç³»ç»Ÿä¸»æ§åˆ¶å™¨
//
// æ¶æ„è®¾è®¡:
// - ä¸»çº¿ç¨‹: è¿è¡Œæ¨¡å‹æ¨ç†æœåŠ¡ (InferenceServer)
// - å·¥ä½œçº¿ç¨‹æ± : æ¯ä¸ªçº¿ç¨‹è¿è¡Œç‹¬ç«‹çš„è‡ªå¯¹å¼ˆæ¸¸æˆ
// - é€šä¿¡: é€šè¿‡ channel å‘é€æ¨ç†è¯·æ±‚å’Œæ¥æ”¶ç»“æœ
// - æ‰¹é‡æ¨ç†: æ”¶é›†å¤šä¸ªè¯·æ±‚åæ‰¹é‡å¤„ç†ï¼Œæé«˜GPUåˆ©ç”¨ç‡
// 
// æ•°æ®å­˜å‚¨:
// - å†…å­˜ç¼“å†²åŒº: æŒ‰æ•´å±€æ¸¸æˆå­˜å‚¨ï¼Œä¿ç•™æœ€è¿‘1000å±€
// - MongoDB: æ¯è½®è®­ç»ƒååŒæ­¥ä¿å­˜æ–°å¢æ¸¸æˆæ•°æ®ï¼ˆæ— å¼‚æ­¥/tokioä¾èµ–ï¼‰

use anyhow::Result;
use banqi_4x8::inference::{ChannelEvaluator, InferenceServer};
use banqi_4x8::mongodb_storage::MongoStorage;
use banqi_4x8::nn_model::BanqiNet;
use banqi_4x8::self_play::{GameEpisode, ScenarioType, SelfPlayWorker};
use banqi_4x8::training::{get_loss_weights, train_step};
use banqi_4x8::training_log::TrainingLog;
use std::env;
use std::sync::{mpsc, Arc};
use std::thread;
use std::time::Instant;
use tch::{nn, nn::OptimizerConfig, Device};

// ================ ä¸»è®­ç»ƒå¾ªç¯ ================

fn main() -> Result<()> {
    if let Err(e) = parallel_train_loop() {
        eprintln!("è®­ç»ƒå¤±è´¥: {}", e);
    }
    Ok(())
}

pub fn parallel_train_loop() -> Result<()> {
    // è®¾å¤‡é…ç½®
    let cuda_available = tch::Cuda::is_available();
    println!("CUDA available: {}", cuda_available);

    let device = if cuda_available {
        println!("Using CUDA device 0");
        Device::Cuda(0)
    } else {
        println!("Using CPU");
        Device::Cpu
    };

    // è®­ç»ƒé…ç½®
    let num_workers = 32; // æ¯ä¸ªåœºæ™¯ä¸€ä¸ªå·¥ä½œçº¿ç¨‹
    let mcts_sims = 800; // MCTSæ¨¡æ‹Ÿæ¬¡æ•°
    let num_iterations = 2000; // è®­ç»ƒè¿­ä»£æ¬¡æ•°
    let num_episodes_per_iteration = 4; // æ¯è½®æ¯ä¸ªåœºæ™¯çš„æ¸¸æˆæ•°
    let inference_batch_size = 28;
    let inference_timeout_ms = 4;
    let batch_size = 256;
    let epochs_per_iteration = 10;
    let max_buffer_games = 1000; // ç¼“å†²åŒºä¿ç•™æœ€è¿‘1000å±€æ¸¸æˆ
    let learning_rate = 1e-4;
    
    // Dirichlet å™ªå£°å‚æ•°ï¼ˆæ›¿ä»£æ¸©åº¦å‚æ•°ï¼‰
    let dirichlet_alpha = 0.3;    // Alphaå€¼ï¼Œæ§åˆ¶å™ªå£°çš„é›†ä¸­åº¦
    let dirichlet_epsilon = 0.25; // å™ªå£°æƒé‡ï¼Œä¸å…ˆéªŒç­–ç•¥çš„æ··åˆæ¯”ä¾‹

    println!("\n=== åœºæ™¯è‡ªå¯¹å¼ˆè®­ç»ƒé…ç½® ===");
    println!("å·¥ä½œçº¿ç¨‹æ•°: {} (å…¨æ ‡å‡†ç¯å¢ƒ)", num_workers);
    println!("æ¯è½®æ¯åœºæ™¯æ¸¸æˆæ•°: {}", num_episodes_per_iteration);
    println!("MCTSæ¨¡æ‹Ÿæ¬¡æ•°: {}", mcts_sims);
    println!("è®­ç»ƒè¿­ä»£æ¬¡æ•°: {}", num_iterations);
    println!("æ¨ç†æ‰¹é‡å¤§å°: {}", inference_batch_size);
    println!("æ¸¸æˆç¼“å†²åŒº: æœ€è¿‘ {} å±€", max_buffer_games);
    println!("Dirichletå™ªå£°: alpha={}, epsilon={}", dirichlet_alpha, dirichlet_epsilon);
    println!("åœºæ™¯: Standard");

    // è¿æ¥MongoDB
    let mongo_uri = env::var("MONGODB_URI").unwrap_or_else(|_| "mongodb://localhost:27017".to_string());
    let mongo_storage = MongoStorage::new(&mongo_uri, "banqi_training", "games")?;
    println!("MongoDBè¿æ¥æˆåŠŸ");


    // åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
    let mut vs = nn::VarStore::new(device);
    let net = BanqiNet::new(&vs.root());
    let mut opt = nn::Adam::default().build(&vs, learning_rate)?;

    // åŠ è½½æ¨¡å‹ (å¦‚æœæä¾›äº†å‚æ•°)
    let args: Vec<String> = env::args().collect();
    if args.len() > 1 {
        let model_path = &args[1];
        println!("æ­£åœ¨åŠ è½½æ¨¡å‹: {}", model_path);
        if let Err(e) = vs.load(model_path) {
            eprintln!("åŠ è½½æ¨¡å‹å¤±è´¥: {}", e);
        } else {
            println!("æ¨¡å‹åŠ è½½æˆåŠŸï¼");
        }
    }

    // æ¸¸æˆç¼“å†²åŒº - æŒ‰æ•´å±€å­˜å‚¨ï¼Œè®­ç»ƒæ—¶ç›´æ¥ä½¿ç”¨ï¼Œé¿å…å…‹éš†
    let mut game_buffer: Vec<GameEpisode> = Vec::new();

    // ä¸»è®­ç»ƒå¾ªç¯
    for iteration in 0..num_iterations {
        println!(
            "\n========== Iteration {}/{} ==========",
            iteration + 1,
            num_iterations
        );

        // ä¿å­˜ä¸´æ—¶æ¨¡å‹ä¾›æ¨ç†æœåŠ¡å™¨ä½¿ç”¨
        let temp_model_path = format!("banqi_model_iter_{}_temp.ot", iteration);
        vs.save(&temp_model_path)?;

        // åˆ›å»ºæ¨ç†é€šé“
        let (req_tx, req_rx) = mpsc::channel();

        // å¯åŠ¨æ¨ç†æœåŠ¡å™¨çº¿ç¨‹
        let temp_model_path_clone = temp_model_path.clone();
        let inference_handle = thread::spawn(move || {
            match InferenceServer::new(
                &temp_model_path_clone,
                device,
                req_rx,
                inference_batch_size,
                inference_timeout_ms,
            ) {
                Ok(server) => server.run(),
                Err(e) => {
                    eprintln!("[InferenceServer] åˆå§‹åŒ–å¤±è´¥: {}", e);
                }
            }
        });

        // å¯åŠ¨å·¥ä½œçº¿ç¨‹ - æ¯ä¸ªåœºæ™¯ä¸€ä¸ª
        let scenarios = vec![ScenarioType::Standard; num_workers];
        let mut worker_handles = Vec::new();
        let mut result_rxs = Vec::new();

        for (worker_id, scenario) in scenarios.iter().enumerate() {
            let req_tx_clone = req_tx.clone();
            let (result_tx, result_rx) = mpsc::channel();
            result_rxs.push(result_rx);
            let scenario_copy = *scenario;
            let alpha = dirichlet_alpha;
            let epsilon = dirichlet_epsilon;

            let handle = thread::spawn(move || {
                let evaluator = Arc::new(ChannelEvaluator::new(req_tx_clone));
                let worker = SelfPlayWorker::with_scenario_and_dirichlet(
                    worker_id,
                    evaluator,
                    mcts_sims,
                    scenario_copy,
                    alpha,
                    epsilon,
                );

                let mut all_episodes = Vec::new();
                for ep in 0..num_episodes_per_iteration {
                    let episode = worker.play_episode(ep);
                    all_episodes.push(episode);
                }

                println!(
                    "  [Worker-{}] å®Œæˆ {} å±€ {} æ¸¸æˆ",
                    worker_id,
                    num_episodes_per_iteration,
                    scenario_copy.name()
                );
                result_tx.send(all_episodes).expect("æ— æ³•å‘é€ç»“æœ");
            });

            worker_handles.push(handle);
        }

        // å…³é—­ä¸»è¯·æ±‚å‘é€ç«¯
        drop(req_tx);

        // æ”¶é›†æ‰€æœ‰å·¥ä½œçº¿ç¨‹çš„ç»“æœ
        let mut all_episodes = Vec::new();
        for result_rx in result_rxs {
            if let Ok(episodes) = result_rx.recv() {
                all_episodes.extend(episodes);
            }
        }

        // ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹å®Œæˆ
        for handle in worker_handles {
            handle.join().expect("å·¥ä½œçº¿ç¨‹å¼‚å¸¸");
        }

        // ç­‰å¾…æ¨ç†æœåŠ¡å™¨é€€å‡º
        inference_handle.join().expect("æ¨ç†æœåŠ¡å™¨å¼‚å¸¸");

        // æ¸…ç†ä¸´æ—¶æ¨¡å‹æ–‡ä»¶ï¼ˆç¡®ä¿åˆ é™¤ï¼‰
        if let Err(e) = std::fs::remove_file(&temp_model_path) {
            eprintln!("  âš ï¸ æ¸…ç†ä¸´æ—¶æ¨¡å‹å¤±è´¥: {} - {}", temp_model_path, e);
        }

        // ä½¿ç”¨æ‰€æœ‰æ¸¸æˆï¼ˆåŒ…æ‹¬å¹³å±€ï¼‰- é¿å…ä¸å¿…è¦çš„å…‹éš†
        println!("  æ”¶é›†äº† {} å±€æ¸¸æˆ", all_episodes.len());

        // æ¯è½®ç«‹å³ä¿å­˜æ–°å¢çš„æ¸¸æˆåˆ°MongoDB
        println!("  æ­£åœ¨ä¿å­˜æ•°æ®åˆ°MongoDB...");
        mongo_storage.save_games(iteration, &all_episodes)?;
        
        // è·å–å¹¶æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if let Ok(stats) = mongo_storage.get_iteration_stats(iteration) {
            stats.print();
        }

        // æ›´æ–°æ¸¸æˆç¼“å†²åŒº - æŒ‰æ•´å±€å­˜å‚¨ï¼ˆç§»åŠ¨æ‰€æœ‰æƒè€Œéå…‹éš†ï¼‰
        game_buffer.extend(all_episodes);
        // all_episodes å·²è¢«ç§»åŠ¨ï¼Œè‡ªåŠ¨é‡Šæ”¾
        
        if game_buffer.len() > max_buffer_games {
            let remove_count = game_buffer.len() - max_buffer_games;
            game_buffer.drain(0..remove_count);
        }
        
        // ç»Ÿè®¡ç¼“å†²åŒºä¿¡æ¯
        let total_samples: usize = game_buffer.iter().map(|ep| ep.samples.len()).sum();
        println!("  æ¸¸æˆç¼“å†²åŒº: {} å±€æ¸¸æˆ, {} ä¸ªæ ·æœ¬", game_buffer.len(), total_samples);

        if total_samples == 0 {
            println!("  âš ï¸ æœ¬è½®æ²¡æœ‰æ”¶é›†åˆ°æœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡è®­ç»ƒ");
            continue;
        }

        // è®­ç»ƒ - ç›´æ¥ä½¿ç”¨ game_bufferï¼Œé¿å…å…‹éš†åˆ° replay_buffer
        println!("  å¼€å§‹è®­ç»ƒ...");
        let train_start = Instant::now();
        let mut loss_sum = 0.0_f64;
        let mut p_loss_sum = 0.0_f64;
        let mut v_loss_sum = 0.0_f64;
        for epoch in 0..epochs_per_iteration {
            let (loss, p_loss, v_loss) =
                train_step(&mut opt, &net, &game_buffer, batch_size, device, epoch);
            println!(
                "    Epoch {}/{}: Loss={:.4} (Policy={:.4}, Value={:.4})",
                epoch + 1,
                epochs_per_iteration,
                loss,
                p_loss,
                v_loss
            );
            loss_sum += loss;
            p_loss_sum += p_loss;
            v_loss_sum += v_loss;
        }
        let train_elapsed = train_start.elapsed();
        println!("  è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ {:.1}s", train_elapsed.as_secs_f64());

        // ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ¯è½®è®­ç»ƒåæ‰‹åŠ¨è§¦å‘å†…å­˜æ¸…ç†
        // é€šè¿‡ sleep ç»™ PyTorch åå°çº¿ç¨‹æ—¶é—´æ¸…ç†ç¼“å­˜
        std::thread::sleep(std::time::Duration::from_millis(50));

        // ========== ç”Ÿæˆè®­ç»ƒæ—¥å¿— ==========
        // ç»Ÿè®¡å¯¹å¼ˆæ•´ä½“æ•°æ®ï¼ˆåŒ…å«å¹³å±€ï¼‰
        // æ³¨æ„ï¼šall_episodeså·²ç»è¢«ç§»åŠ¨åˆ°game_bufferï¼Œä½¿ç”¨æœ€è¿‘çš„æ¸¸æˆæ•°æ®
        let recent_episodes_count = (num_workers * num_episodes_per_iteration).min(game_buffer.len());
        let recent_episodes = &game_buffer[(game_buffer.len() - recent_episodes_count)..];
        
        let total_games = recent_episodes.len().max(1);
        let red_wins = recent_episodes
            .iter()
            .filter(|ep| ep.winner == Some(1))
            .count();
        let black_wins = recent_episodes
            .iter()
            .filter(|ep| ep.winner == Some(-1))
            .count();
        let draws = recent_episodes
            .iter()
            .filter(|ep| ep.winner.is_none() || ep.winner == Some(0))
            .count();
        let avg_steps: f32 = recent_episodes
            .iter()
            .map(|ep| ep.game_length as f32)
            .sum::<f32>()
            / total_games as f32;

        // é’ˆå¯¹æœ¬è½®æ–°æ ·æœ¬çš„ç­–ç•¥ç†µä¸é«˜ç½®ä¿¡åº¦æ¯”ç‡
        let (avg_entropy, high_conf_ratio) = if !recent_episodes.is_empty() {
            let mut ent_sum = 0.0_f32;
            let mut count = 0usize;
            let mut high_conf = 0usize;
            for ep in recent_episodes {
                for (_, probs, _, _, _) in &ep.samples {
                    // é¿å…ln(0)
                    let mut e = 0.0_f32;
                    let mut maxp = 0.0_f32;
                    for &p in probs {
                        if p > 1e-8 {
                            e += -p * p.ln();
                        }
                        if p > maxp {
                            maxp = p;
                        }
                    }
                    ent_sum += e;
                    count += 1;
                    if maxp >= 0.90 {
                        high_conf += 1;
                    }
                }
            }
            if count > 0 {
                (ent_sum / count as f32, high_conf as f32 / count as f32)
            } else {
                (0.0, 0.0)
            }
        } else {
            (0.0, 0.0)
        };

        let avg_total_loss = loss_sum / epochs_per_iteration as f64;
        let avg_policy_loss = p_loss_sum / epochs_per_iteration as f64;
        let avg_value_loss = v_loss_sum / epochs_per_iteration as f64;
        let (plw, vlw) = get_loss_weights(epochs_per_iteration.saturating_sub(1));

        // ç”Ÿæˆè®­ç»ƒæ—¥å¿—ï¼ˆç§»é™¤åœºæ™¯éªŒè¯å­—æ®µï¼‰
        let log_record = TrainingLog {
            iteration,
            avg_total_loss,
            avg_policy_loss,
            avg_value_loss,
            policy_loss_weight: plw,
            value_loss_weight: vlw,
            // åœºæ™¯éªŒè¯å·²ç§»é™¤ï¼Œä½¿ç”¨é»˜è®¤å€¼
            scenario1_value: 0.0,
            scenario1_unmasked_a38: 0.0,
            scenario1_unmasked_a39: 0.0,
            scenario1_unmasked_a40: 0.0,
            scenario1_masked_a38: 0.0,
            scenario1_masked_a39: 0.0,
            scenario1_masked_a40: 0.0,
            scenario2_value: 0.0,
            scenario2_unmasked_a3: 0.0,
            scenario2_unmasked_a5: 0.0,
            scenario2_masked_a3: 0.0,
            scenario2_masked_a5: 0.0,
            // æ ·æœ¬ç»Ÿè®¡
            new_samples_count: recent_episodes
                .iter()
                .map(|ep| ep.samples.len())
                .sum::<usize>(),
            replay_buffer_size: total_samples,
            avg_game_steps: avg_steps,
            red_win_ratio: red_wins as f32 / total_games as f32,
            draw_ratio: draws as f32 / total_games as f32,
            black_win_ratio: black_wins as f32 / total_games as f32,
            avg_policy_entropy: avg_entropy,
            high_confidence_ratio: high_conf_ratio,
        };

        // å†™å…¥CSV
        let csv_path = "training_log.csv";
        let _ = TrainingLog::write_header(csv_path);
        if let Err(e) = log_record.append_to_csv(csv_path) {
            eprintln!("  âš ï¸ å†™å…¥è®­ç»ƒæ—¥å¿—å¤±è´¥: {}", e);
        } else {
            println!("  å·²è®°å½•è®­ç»ƒæ—¥å¿—åˆ° {}", csv_path);
        }

        // ä¿å­˜æ¨¡å‹
        if (iteration + 1) % 5 == 0 || iteration == num_iterations - 1 {
            let model_path = format!("banqi_model_{}.ot", iteration + 1);
            vs.save(&model_path)?;
            println!("  å·²ä¿å­˜æ¨¡å‹: {}", model_path);
        }
        
        // è¿­ä»£ç»“æŸæ—¶æ¸…ç†æœ¬æ¬¡å¾ªç¯çš„å˜é‡
        // recent_episodes æ˜¯åˆ‡ç‰‡å¼•ç”¨ï¼Œä¸éœ€è¦ drop
        // log_record ä¼šè‡ªåŠ¨é‡Šæ”¾
        
        println!("  ======== Iteration {} å®Œæˆ ========\n", iteration + 1);
    }

    // ä¿å­˜æœ€ç»ˆæ¨¡å‹
    vs.save("banqi_model_latest.ot")?;
    println!("\nè®­ç»ƒå®Œæˆï¼å·²ä¿å­˜æ¨¡å‹: banqi_model_latest.ot");
    println!("\nè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•æ¨¡å‹:");
    println!("  cargo run --bin banqi-verify-trained -- banqi_model_latest.ot");

    Ok(())
}
