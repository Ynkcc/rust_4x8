// lr_finder.rs - å­¦ä¹ ç‡æ‰«æå™¨ (Learning Rate Finder)
//
// å®ç°å­¦ä¹ ç‡èŒƒå›´æµ‹è¯• (LR Range Test)ï¼Œå¸®åŠ©æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡ã€‚
// ç®—æ³•åŸºäº Leslie Smith çš„è®ºæ–‡ "Cyclical Learning Rates for Training Neural Networks"
//
// ä½¿ç”¨æ–¹æ³•:
// 1. å‡†å¤‡ä¸€ä¸ªè®­ç»ƒæ ·æœ¬é›†ï¼ˆä»æ•°æ®åº“æˆ–è‡ªå¯¹å¼ˆè·å–ï¼‰
// 2. è°ƒç”¨ `find_learning_rate()` æ‰§è¡Œæ‰«æ
// 3. æŸ¥çœ‹ç”Ÿæˆçš„ `lr_finder_results.csv` æ–‡ä»¶
// 4. ç»˜åˆ¶å­¦ä¹ ç‡-æŸå¤±æ›²çº¿ï¼Œæ‰¾åˆ°æŸå¤±ä¸‹é™æœ€å¿«çš„åŒºé—´
//
// æ¨èå­¦ä¹ ç‡é€‰æ‹©ç­–ç•¥:
// - æœ€å°å­¦ä¹ ç‡: æŸå¤±å¼€å§‹ä¸‹é™çš„ä½ç½®
// - æœ€å¤§å­¦ä¹ ç‡: æŸå¤±è¾¾åˆ°æœ€ä½ç‚¹ä¹‹å‰ï¼ˆé¿å…å‘æ•£ï¼‰
// - åˆå§‹å­¦ä¹ ç‡: é€šå¸¸é€‰æ‹©æœ€å°å€¼çš„ 3-10 å€

use crate::nn_model::BanqiNet;
use crate::game_env::Observation;
use anyhow::Result;
use tch::{nn, nn::OptimizerConfig, Device, Tensor, Kind};
use std::fs::File;
use std::io::Write;

/// å­¦ä¹ ç‡æ‰«æç»“æœ
#[derive(Debug, Clone)]
pub struct LRFinderResult {
    pub learning_rate: f64,
    pub loss: f64,
    pub policy_loss: f64,
    pub value_loss: f64,
}

/// å­¦ä¹ ç‡æ‰«æå™¨é…ç½®
pub struct LRFinderConfig {
    /// èµ·å§‹å­¦ä¹ ç‡ï¼ˆé€šå¸¸å¾ˆå°ï¼Œå¦‚ 1e-8ï¼‰
    pub start_lr: f64,
    /// ç»“æŸå­¦ä¹ ç‡ï¼ˆé€šå¸¸è¾ƒå¤§ï¼Œå¦‚ 10.0 æˆ– 1.0ï¼‰
    pub end_lr: f64,
    /// æ‰«ææ­¥æ•°ï¼ˆå­¦ä¹ ç‡é‡‡æ ·ç‚¹æ•°é‡ï¼‰
    pub num_steps: usize,
    /// æ¯ä¸ªå­¦ä¹ ç‡è®­ç»ƒçš„æ‰¹æ¬¡æ•°ï¼ˆé€šå¸¸ 1-3ï¼‰
    pub num_batches_per_step: usize,
    /// æ‰¹é‡å¤§å°
    pub batch_size: usize,
    /// æŸå¤±å¹³æ»‘çª—å£å¤§å°ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
    pub smooth_window: usize,
    /// æŸå¤±å‘æ•£é˜ˆå€¼å€æ•°ï¼ˆå¦‚æœæŸå¤±è¶…è¿‡æœ€å°æŸå¤±çš„æ­¤å€æ•°ï¼Œæå‰åœæ­¢ï¼‰
    pub divergence_threshold: f64,
}

impl Default for LRFinderConfig {
    fn default() -> Self {
        Self {
            start_lr: 1e-7,
            end_lr: 1.0,
            num_steps: 100,
            num_batches_per_step: 2,
            batch_size: 64,
            smooth_window: 5,
            divergence_threshold: 4.0,
        }
    }
}

/// æ‰§è¡Œå­¦ä¹ ç‡æ‰«æ
///
/// # å‚æ•°
/// - `model`: è¦æµ‹è¯•çš„ç¥ç»ç½‘ç»œæ¨¡å‹
/// - `examples`: è®­ç»ƒæ ·æœ¬é›† (è§‚å¯Ÿ, ç­–ç•¥æ¦‚ç‡, ä»·å€¼ç›®æ ‡, åŠ¨ä½œæ©ç )
/// - `device`: è®­ç»ƒè®¾å¤‡ (CPU/GPU)
/// - `config`: å­¦ä¹ ç‡æ‰«æé…ç½®
///
/// # è¿”å›
/// å­¦ä¹ ç‡æ‰«æç»“æœå‘é‡ï¼ŒåŒ…å«æ¯ä¸ªå­¦ä¹ ç‡ä¸‹çš„æŸå¤±å€¼
pub fn find_learning_rate(
    model: &BanqiNet,
    examples: &[(Observation, Vec<f32>, f32, Vec<i32>)],
    device: Device,
    config: &LRFinderConfig,
) -> Result<Vec<LRFinderResult>> {
    if examples.is_empty() {
        anyhow::bail!("è®­ç»ƒæ ·æœ¬é›†ä¸ºç©º");
    }
    
    if examples.len() < config.batch_size {
        anyhow::bail!("æ ·æœ¬æ•°é‡ ({}) å°‘äºæ‰¹é‡å¤§å° ({})", examples.len(), config.batch_size);
    }
    
    println!("\n========== å­¦ä¹ ç‡æ‰«æå™¨ ==========");
    println!("é…ç½®:");
    println!("  å­¦ä¹ ç‡èŒƒå›´: {:.2e} -> {:.2e}", config.start_lr, config.end_lr);
    println!("  æ‰«ææ­¥æ•°: {}", config.num_steps);
    println!("  æ¯æ­¥æ‰¹æ¬¡æ•°: {}", config.num_batches_per_step);
    println!("  æ‰¹é‡å¤§å°: {}", config.batch_size);
    println!("  æ ·æœ¬æ€»æ•°: {}", examples.len());
    println!("  å¹³æ»‘çª—å£: {}", config.smooth_window);
    println!("  å‘æ•£é˜ˆå€¼: {}x", config.divergence_threshold);
    
    // åˆ›å»ºæ¨¡å‹çš„å‰¯æœ¬ï¼ˆç”¨äºæ‰«æï¼Œä¸å½±å“åŸæ¨¡å‹ï¼‰
    let mut vs = nn::VarStore::new(device);
    let test_net = BanqiNet::new(&vs.root());
    
    // å¤åˆ¶æ¨¡å‹å‚æ•°
    // æ³¨æ„: è¿™é‡Œå‡è®¾åŸæ¨¡å‹å·²ç»æœ‰ä¸€äº›é¢„è®­ç»ƒæƒé‡
    // å¦‚æœä»å¤´å¼€å§‹ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€æ­¥
    
    // è®¡ç®—å­¦ä¹ ç‡çš„å¯¹æ•°é—´éš”
    let log_start = config.start_lr.ln();
    let log_end = config.end_lr.ln();
    let log_step = (log_end - log_start) / (config.num_steps - 1) as f64;
    
    let mut results = Vec::new();
    let mut min_loss = f64::MAX;
    let mut loss_history = Vec::new();
    
    // ç­–ç•¥å’Œä»·å€¼æŸå¤±æƒé‡ï¼ˆå¯ä»¥è°ƒæ•´ï¼‰
    let policy_weight = 1.0;
    let value_weight = 1.0;
    
    println!("\nå¼€å§‹æ‰«æ...");
    
    for step in 0..config.num_steps {
        // è®¡ç®—å½“å‰å­¦ä¹ ç‡ï¼ˆæŒ‡æ•°å¢é•¿ï¼‰
        let lr = (log_start + step as f64 * log_step).exp();
        
        // åˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨å½“å‰å­¦ä¹ ç‡ï¼‰
        let mut opt = nn::Adam::default().build(&vs, lr)?;
        
        let mut step_loss_sum = 0.0;
        let mut step_policy_loss_sum = 0.0;
        let mut step_value_loss_sum = 0.0;
        let mut num_batches = 0;
        
        // åœ¨å½“å‰å­¦ä¹ ç‡ä¸‹è®­ç»ƒå¤šä¸ªæ‰¹æ¬¡
        for batch_idx in 0..config.num_batches_per_step {
            // éšæœºé€‰æ‹©ä¸€ä¸ªæ‰¹æ¬¡
            let batch_start = (step * config.num_batches_per_step + batch_idx) * config.batch_size;
            let batch_start = batch_start % (examples.len() - config.batch_size);
            let batch = &examples[batch_start..batch_start + config.batch_size];
            
            // å‡†å¤‡æ‰¹é‡æ•°æ®
            let (board_tensor, scalar_tensor, target_p, target_v, mask_tensor) = 
                prepare_batch(batch, device);
            
            // å‰å‘ä¼ æ’­
            let (logits, value) = test_net.forward(&board_tensor, &scalar_tensor);
            let masked_logits = &logits + (&mask_tensor - 1.0) * 1e9;
            let log_probs = masked_logits.log_softmax(-1, Kind::Float);
            
            // è®¡ç®—æŸå¤±
            let reduce_dim = [-1i64];
            let p_loss = (&target_p * &log_probs)
                .sum_dim_intlist(&reduce_dim[..], false, Kind::Float)
                .mean(Kind::Float)
                .neg() * policy_weight;
            let v_loss = value.mse_loss(&target_v, tch::Reduction::Mean) * value_weight;
            let total_loss = &p_loss + &v_loss;
            
            // åå‘ä¼ æ’­å’Œæ›´æ–°
            opt.backward_step(&total_loss);
            
            // è®°å½•æŸå¤±
            step_loss_sum += total_loss.double_value(&[]);
            step_policy_loss_sum += p_loss.double_value(&[]) / policy_weight;
            step_value_loss_sum += v_loss.double_value(&[]) / value_weight;
            num_batches += 1;
        }
        
        // è®¡ç®—å¹³å‡æŸå¤±
        let avg_loss = step_loss_sum / num_batches as f64;
        let avg_policy_loss = step_policy_loss_sum / num_batches as f64;
        let avg_value_loss = step_value_loss_sum / num_batches as f64;
        
        // åº”ç”¨å¹³æ»‘ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
        loss_history.push(avg_loss);
        let smooth_loss = if loss_history.len() >= config.smooth_window {
            let start_idx = loss_history.len() - config.smooth_window;
            loss_history[start_idx..].iter().sum::<f64>() / config.smooth_window as f64
        } else {
            loss_history.iter().sum::<f64>() / loss_history.len() as f64
        };
        
        // è®°å½•ç»“æœ
        let result = LRFinderResult {
            learning_rate: lr,
            loss: smooth_loss,
            policy_loss: avg_policy_loss,
            value_loss: avg_value_loss,
        };
        results.push(result.clone());
        
        // æ›´æ–°æœ€å°æŸå¤±
        if smooth_loss < min_loss {
            min_loss = smooth_loss;
        }
        
        // æ‰“å°è¿›åº¦
        if step % 10 == 0 || step == config.num_steps - 1 {
            println!("  Step {}/{}: LR={:.2e}, Loss={:.4} (Policy={:.4}, Value={:.4})", 
                step + 1, config.num_steps, lr, smooth_loss, avg_policy_loss, avg_value_loss);
        }
        
        // æ£€æŸ¥æ˜¯å¦å‘æ•£ï¼ˆæŸå¤±æš´å¢ï¼‰
        if smooth_loss > min_loss * config.divergence_threshold && min_loss > 0.0 {
            println!("\nâš ï¸ æ£€æµ‹åˆ°æŸå¤±å‘æ•£ (å½“å‰={:.4}, æœ€å°={:.4}, é˜ˆå€¼={}x)", 
                smooth_loss, min_loss, config.divergence_threshold);
            println!("æå‰åœæ­¢æ‰«æã€‚");
            break;
        }
    }
    
    println!("\næ‰«æå®Œæˆï¼å…±é‡‡é›† {} ä¸ªæ•°æ®ç‚¹", results.len());
    
    // ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶
    save_results_to_csv(&results, "lr_finder_results.csv")?;
    println!("ç»“æœå·²ä¿å­˜åˆ°: lr_finder_results.csv");
    
    // åˆ†æå¹¶ç»™å‡ºå»ºè®®
    analyze_and_suggest(&results)?;
    
    Ok(results)
}

/// å‡†å¤‡ä¸€ä¸ªæ‰¹æ¬¡çš„è®­ç»ƒæ•°æ®
fn prepare_batch(
    batch: &[(Observation, Vec<f32>, f32, Vec<i32>)],
    device: Device,
) -> (Tensor, Tensor, Tensor, Tensor, Tensor) {
    let bsz = batch.len();
    let mut board_buf = Vec::with_capacity(bsz * 8 * 3 * 4);
    let mut scalar_buf = Vec::with_capacity(bsz * 56);
    let mut target_prob_buf = Vec::with_capacity(bsz * 46);
    let mut target_val_buf = Vec::with_capacity(bsz);
    let mut mask_buf = Vec::with_capacity(bsz * 46);
    
    for (obs, target_probs, target_val, masks) in batch.iter() {
        let board_slice = obs.board.as_slice().expect("board slice");
        board_buf.extend_from_slice(board_slice);
        let scalar_slice = obs.scalars.as_slice().expect("scalar slice");
        scalar_buf.extend_from_slice(scalar_slice);
        target_prob_buf.extend_from_slice(target_probs);
        target_val_buf.push(*target_val);
        mask_buf.extend(masks.iter().map(|&m| m as f32));
    }
    
    let board_tensor = Tensor::from_slice(&board_buf)
        .view([bsz as i64, 8, 3, 4])
        .to(device);
    let scalar_tensor = Tensor::from_slice(&scalar_buf)
        .view([bsz as i64, 56])
        .to(device);
    let target_p = Tensor::from_slice(&target_prob_buf)
        .view([bsz as i64, 46])
        .to(device);
    let target_v = Tensor::from_slice(&target_val_buf)
        .view([bsz as i64, 1])
        .to(device);
    let mask_tensor = Tensor::from_slice(&mask_buf)
        .view([bsz as i64, 46])
        .to(device);
    
    (board_tensor, scalar_tensor, target_p, target_v, mask_tensor)
}

/// ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶
fn save_results_to_csv(results: &[LRFinderResult], path: &str) -> Result<()> {
    let mut file = File::create(path)?;
    
    // å†™å…¥è¡¨å¤´
    writeln!(file, "learning_rate,loss,policy_loss,value_loss")?;
    
    // å†™å…¥æ•°æ®
    for result in results {
        writeln!(file, "{:.8e},{:.6},{:.6},{:.6}", 
            result.learning_rate, 
            result.loss, 
            result.policy_loss, 
            result.value_loss)?;
    }
    
    Ok(())
}

/// åˆ†æç»“æœå¹¶ç»™å‡ºå­¦ä¹ ç‡å»ºè®®
fn analyze_and_suggest(results: &[LRFinderResult]) -> Result<()> {
    if results.is_empty() {
        return Ok(());
    }
    
    println!("\n========== åˆ†æç»“æœ ==========");
    
    // æ‰¾åˆ°æœ€å°æŸå¤±ç‚¹
    let min_loss_idx = results.iter()
        .enumerate()
        .min_by(|(_, a), (_, b)| a.loss.partial_cmp(&b.loss).unwrap())
        .map(|(idx, _)| idx)
        .unwrap();
    
    let min_loss_lr = results[min_loss_idx].learning_rate;
    let min_loss = results[min_loss_idx].loss;
    
    println!("æœ€å°æŸå¤±ç‚¹:");
    println!("  å­¦ä¹ ç‡: {:.2e}", min_loss_lr);
    println!("  æŸå¤±: {:.4}", min_loss);
    
    // æ‰¾åˆ°æŸå¤±ä¸‹é™æœ€å¿«çš„åŒºé—´ï¼ˆæ¢¯åº¦æœ€è´Ÿï¼‰
    let mut max_gradient = 0.0_f64;
    let mut max_gradient_idx = 0;
    
    for i in 1..results.len() {
        let lr_diff = results[i].learning_rate.ln() - results[i-1].learning_rate.ln();
        let loss_diff = results[i].loss - results[i-1].loss;
        let gradient = loss_diff / lr_diff; // d(loss)/d(log_lr)
        
        if gradient < max_gradient {
            max_gradient = gradient;
            max_gradient_idx = i;
        }
    }
    
    let steepest_lr = results[max_gradient_idx].learning_rate;
    
    println!("\næŸå¤±ä¸‹é™æœ€å¿«åŒºé—´:");
    println!("  å­¦ä¹ ç‡: {:.2e}", steepest_lr);
    println!("  æ¢¯åº¦: {:.4}", max_gradient);
    
    // ç»™å‡ºå»ºè®®
    println!("\n========== å­¦ä¹ ç‡å»ºè®® ==========");
    println!("ğŸ“Š åˆ†ææ–¹æ³•:");
    println!("  1. ç»˜åˆ¶å­¦ä¹ ç‡-æŸå¤±æ›²çº¿: ä½¿ç”¨ lr_finder_results.csv");
    println!("  2. æ‰¾åˆ°æŸå¤±ä¸‹é™æœ€é™¡çš„åŒºåŸŸï¼ˆæ›²çº¿æ–œç‡æœ€è´Ÿï¼‰");
    println!("  3. åœ¨è¯¥åŒºåŸŸçš„èµ·ç‚¹å’ŒæŸå¤±æœ€ä½ç‚¹ä¹‹é—´é€‰æ‹©å­¦ä¹ ç‡");
    
    println!("\nğŸ’¡ æ¨èå­¦ä¹ ç‡èŒƒå›´:");
    
    // ä¿å®ˆå»ºè®®: æœ€é™¡ç‚¹åˆ°æœ€å°æŸå¤±ç‚¹ä¹‹é—´
    let suggested_min_lr = steepest_lr;
    let suggested_max_lr = min_loss_lr / 3.0; // æœ€å°æŸå¤±ç‚¹çš„ 1/3ï¼Œé¿å…å‘æ•£
    let suggested_initial_lr = (suggested_min_lr * suggested_max_lr).sqrt(); // å‡ ä½•å¹³å‡
    
    println!("  åˆå§‹å­¦ä¹ ç‡: {:.2e}", suggested_initial_lr);
    println!("  æœ€å°å­¦ä¹ ç‡: {:.2e} (ç”¨äºå­¦ä¹ ç‡è°ƒåº¦)", suggested_min_lr);
    println!("  æœ€å¤§å­¦ä¹ ç‡: {:.2e} (ç”¨äºå¾ªç¯å­¦ä¹ ç‡)", suggested_max_lr);
    
    println!("\nğŸ“ˆ ä½¿ç”¨å»ºè®®:");
    println!("  - å•ä¸€å­¦ä¹ ç‡: ä½¿ç”¨åˆå§‹å­¦ä¹ ç‡ {:.2e}", suggested_initial_lr);
    println!("  - å­¦ä¹ ç‡è¡°å‡: ä» {:.2e} å¼€å§‹ï¼Œé€æ­¥é™ä½", suggested_initial_lr);
    println!("  - å¾ªç¯å­¦ä¹ ç‡: åœ¨ {:.2e} å’Œ {:.2e} ä¹‹é—´å¾ªç¯", suggested_min_lr, suggested_max_lr);
    println!("  - Adam ä¼˜åŒ–å™¨: å½“å‰ä½¿ç”¨ Adamï¼Œå»ºè®®èµ·å§‹å­¦ä¹ ç‡ {:.2e}", suggested_initial_lr);
    
    println!("\nâš ï¸ æ³¨æ„äº‹é¡¹:");
    println!("  - è¿™äº›æ˜¯å»ºè®®å€¼ï¼Œå®é™…è®­ç»ƒæ—¶éœ€è¦æ ¹æ®éªŒè¯é›†è¡¨ç°è°ƒæ•´");
    println!("  - å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼Œé™ä½å­¦ä¹ ç‡ï¼ˆé™¤ä»¥ 2-10ï¼‰");
    println!("  - å¦‚æœæ”¶æ•›å¤ªæ…¢ï¼Œå¯ä»¥å°è¯•ç¨å¾®å¢å¤§å­¦ä¹ ç‡");
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_lr_finder_config_default() {
        let config = LRFinderConfig::default();
        assert_eq!(config.start_lr, 1e-7);
        assert_eq!(config.end_lr, 1.0);
        assert_eq!(config.num_steps, 100);
    }
}
