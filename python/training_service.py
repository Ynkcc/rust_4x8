import time
import os
import torch
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from pymongo import MongoClient
import random

# å¼•å…¥ä½ çš„æ¨¡å‹å®šä¹‰
from nn_model import BanqiNet
from constant import (
    TOTAL_INPUT_CHANNELS,
    BOARD_ROWS,
    BOARD_COLS,
    SCALAR_FEATURE_COUNT,
    ACTION_SPACE_SIZE
)

# --- Configuration ---
MONGO_URI = os.getenv("MONGODB_URI", "mongodb://localhost:27017")
DB_NAME = "banqi_training"
COLLECTION_NAME = "games"
META_COLLECTION = "training_meta"              # ç”¨äºæŒä¹…åŒ–è®­ç»ƒè¿›åº¦
MODEL_PATH = "banqi_model_latest.pt"           # TorchScript æ¨¡å‹ï¼Œä¾› Rust åŠ è½½
STATE_DICT_PATH = "banqi_model_latest.pth"     # State Dictï¼Œä¾› Python è®­ç»ƒ
BATCH_SIZE = 512            # é€‚å½“å¢å¤§ Batch Size ä»¥ç¨³å®šæ¢¯åº¦
LEARNING_RATE = 2e-4        # ç•¥å¾®è°ƒæ•´å­¦ä¹ ç‡
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Buffer é…ç½®
MAX_SAMPLE_BUFFER_SIZE = 50000  # å¢åŠ ç¼“å†²åŒºå®¹é‡
MIN_SAMPLES_TO_START = 2000
FETCH_LIMIT = 500                # æ¯æ¬¡æ‹‰å–çš„æ¸¸æˆæ•°
MAX_STEPS_PER_ROUND = 100        # æ¯è½®æœ€å¤§è®­ç»ƒæ­¥æ•°

# MongoDB å®¢æˆ·ç«¯å•ä¾‹
_mongo_client = None
_mongo_db = None

def get_mongo_db():
    """è·å– MongoDB æ•°æ®åº“è¿æ¥"""
    global _mongo_client, _mongo_db
    if _mongo_client is None:
        _mongo_client = MongoClient(MONGO_URI)
        _mongo_db = _mongo_client[DB_NAME]
    return _mongo_db

def get_mongo_collection():
    """è·å– MongoDB é›†åˆ,å¤ç”¨å®¢æˆ·ç«¯è¿æ¥"""
    return get_mongo_db()[COLLECTION_NAME]

class DataBuffer:
    """å‘é‡åŒ–ç¼“å†²åŒºï¼Œä¼˜åŒ–å†…å­˜å¹¶åŠ é€Ÿ Tensor è½¬æ¢"""
    def __init__(self, capacity):
        self.capacity = capacity
        self.boards = []
        self.scalars = []
        self.probs = []
        self.values = []
        self.masks = []

    def add_samples(self, samples):
        """æ‰¹é‡æ·»åŠ æ ·æœ¬åˆ°ç¼“å†²åŒº"""
        for s in samples:
            # ç¡®ä¿ board_state æ˜¯æ­£ç¡®çš„ 4D å½¢çŠ¶: [Channels, Rows, Cols]
            board = np.array(s['board_state'], dtype=np.float32).reshape(
                TOTAL_INPUT_CHANNELS, BOARD_ROWS, BOARD_COLS
            )
            self.boards.append(board)
            self.scalars.append(np.array(s['scalar_state'], dtype=np.float32))
            self.probs.append(np.array(s['policy_probs'], dtype=np.float32))
            # ä¼˜å…ˆä½¿ç”¨çœŸå®æ¸¸æˆç»“æœ
            val = s.get('game_result_value', s.get('mcts_value', 0.0))
            self.values.append(val)
            self.masks.append(np.array(s['action_mask'], dtype=np.float32))
        
        # FIFO æ·˜æ±°æ—§æ•°æ®
        if len(self.boards) > self.capacity:
            excess = len(self.boards) - self.capacity
            self.boards = self.boards[excess:]
            self.scalars = self.scalars[excess:]
            self.probs = self.probs[excess:]
            self.values = self.values[excess:]
            self.masks = self.masks[excess:]

    def __len__(self):
        return len(self.boards)

    def get_batch(self, indices):
        """å¿«é€Ÿæå–æ‰¹æ¬¡æ•°æ®å¹¶æ„å»º Tensor"""
        b = torch.from_numpy(np.stack([self.boards[i] for i in indices]))
        s = torch.from_numpy(np.stack([self.scalars[i] for i in indices]))
        p = torch.from_numpy(np.stack([self.probs[i] for i in indices]))
        v = torch.tensor([self.values[i] for i in indices], dtype=torch.float32)
        m = torch.from_numpy(np.stack([self.masks[i] for i in indices]))
        return b, s, p, v, m

def get_last_processed_id(db):
    """ä»æ•°æ®åº“è·å–ä¸Šæ¬¡è®­ç»ƒåˆ°çš„æ¸¸æˆ ID"""
    meta = db[META_COLLECTION].find_one({"type": "progress"})
    return meta['last_id'] if meta else None

def save_progress(db, last_id):
    """æŒä¹…åŒ–è®­ç»ƒè¿›åº¦åˆ°æ•°æ®åº“"""
    db[META_COLLECTION].update_one(
        {"type": "progress"},
        {"$set": {"last_id": last_id, "updated_at": time.time()}},
        upsert=True
    )

def save_model(model):
    """
    ä¿å­˜æ¨¡å‹ä¸ºä¸¤ç§æ ¼å¼ï¼š
    1. .pth (state_dict) - ç”¨äº Python è®­ç»ƒæ¢å¤
    2. .pt (TorchScript) - ä¾› Rust æ¨ç†åŠ è½½
    """
    pt_temp_path = MODEL_PATH + ".tmp"
    pth_temp_path = STATE_DICT_PATH + ".tmp"
    
    try:
        model.eval()
        
        # 1. ä¿å­˜ State Dict (.pth)
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_config': {
                'input_channels': TOTAL_INPUT_CHANNELS,
                'board_rows': BOARD_ROWS,
                'board_cols': BOARD_COLS,
                'scalar_features': SCALAR_FEATURE_COUNT,
                'action_space': ACTION_SPACE_SIZE
            }
        }, pth_temp_path)
        os.replace(pth_temp_path, STATE_DICT_PATH)
        
        # 2. ä¿å­˜ TorchScript (.pt)
        with torch.no_grad():
            # åˆ›å»ºç¤ºä¾‹è¾“å…¥ç”¨äº Tracing
            # å¿…é¡»ä¸ Rust ç«¯ tensor ç»´åº¦å®Œå…¨ä¸€è‡´: 
            # Board: [1, 16, 4, 8], Scalars: [1, 242]
            example_board = torch.randn(1, TOTAL_INPUT_CHANNELS, BOARD_ROWS, BOARD_COLS, device=DEVICE)
            example_scalars = torch.randn(1, SCALAR_FEATURE_COUNT, device=DEVICE)
            
            # ä½¿ç”¨ Trace å¯¼å‡º TorchScript
            traced_model = torch.jit.trace(model, (example_board, example_scalars))
            traced_model.save(pt_temp_path)
            
        # åŸå­æ€§æ›¿æ¢ï¼Œé˜²æ­¢è¯»å–åˆ°æŸåæ–‡ä»¶
        os.replace(pt_temp_path, MODEL_PATH)
        
        print(f"[Training] âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {STATE_DICT_PATH} (è®­ç»ƒ) + {MODEL_PATH} (æ¨ç†)")
    except Exception as e:
        print(f"[Training] âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for tmp in [pt_temp_path, pth_temp_path]:
            if os.path.exists(tmp):
                os.remove(tmp)

def train_step(model, optimizer, batch_data, device):
    """
    æ‰§è¡Œå•æ­¥è®­ç»ƒ
    
    Args:
        batch_data: (boards, scalars, target_probs, target_values, masks) å…ƒç»„
    
    Logic:
    1. Policy Loss: CrossEntropy(Network_Logits, Improved_Policy_Target)
       - Improved_Policy_Target æ¥è‡ª Rust ç«¯çš„ Gumbel æœç´¢ç»“æœ
    2. Value Loss: MSE(Network_Value, Game_Result)
       - Game_Result æ˜¯çœŸå®èƒœè´Ÿ (1, -1, 0)ï¼Œè€Œé MCTS ä¼°å€¼
    """
    model.train()
    
    boards_t, scalars_t, target_probs_t, target_values_t, masks_t = batch_data
    
    # æ¬è¿åˆ°è®¾å¤‡
    boards_t = boards_t.to(device)
    scalars_t = scalars_t.to(device)
    target_probs_t = target_probs_t.to(device)
    target_values_t = target_values_t.to(device).view(-1, 1)
    masks_t = masks_t.to(device)

    # å‰å‘ä¼ æ’­
    optimizer.zero_grad()
    logits, values = model(boards_t, scalars_t)

    # Policy Loss (Cross Entropy with Mask)
    masked_logits = logits + (masks_t - 1.0) * 1e9
    log_probs = F.log_softmax(masked_logits, dim=1)
    policy_loss = -torch.sum(target_probs_t * log_probs, dim=1).mean()

    # Value Loss (MSE)
    value_loss = F.mse_loss(values, target_values_t)

    # æ€»æŸå¤±
    total_loss = policy_loss + value_loss

    # åå‘ä¼ æ’­
    total_loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()

    return total_loss.item(), policy_loss.item(), value_loss.item()

def main():
    print(f"[Training] Starting service on {DEVICE}")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = BanqiNet().to(DEVICE)
    
    # ä¼˜å…ˆåŠ è½½ .pth (state_dict)ï¼Œæ›´é€‚åˆç»§ç»­è®­ç»ƒ
    if os.path.exists(STATE_DICT_PATH):
        try:
            checkpoint = torch.load(STATE_DICT_PATH, map_location=DEVICE)
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"[Training] âœ… ä» {STATE_DICT_PATH} åŠ è½½æ¨¡å‹æƒé‡")
        except Exception as e:
            print(f"[Training] âš ï¸ åŠ è½½ .pth å¤±è´¥ ({e})ï¼Œå°è¯• .pt...")
            # å›é€€ï¼šå°è¯•ä» TorchScript åŠ è½½
            if os.path.exists(MODEL_PATH):
                try:
                    jit_model = torch.jit.load(MODEL_PATH, map_location=DEVICE)
                    model.load_state_dict(jit_model.state_dict())
                    print(f"[Training] âœ… ä» {MODEL_PATH} åŠ è½½æ¨¡å‹æƒé‡ (TorchScript å›é€€)")
                except Exception as e2:
                    print(f"[Training] âš ï¸ åŠ è½½å¤±è´¥ ({e2})ï¼Œä½¿ç”¨å…¨æ–°æ¨¡å‹")
    elif os.path.exists(MODEL_PATH):
        # åªæœ‰ .pt å­˜åœ¨
        try:
            jit_model = torch.jit.load(MODEL_PATH, map_location=DEVICE)
            model.load_state_dict(jit_model.state_dict())
            print(f"[Training] âœ… ä» {MODEL_PATH} åŠ è½½æ¨¡å‹æƒé‡")
        except Exception as e:
            print(f"[Training] âš ï¸ åŠ è½½å¤±è´¥ ({e})ï¼Œä½¿ç”¨å…¨æ–°æ¨¡å‹")
    else:
        print("[Training] ğŸ“ åˆ›å»ºå…¨æ–°æ¨¡å‹")
    
    # ç«‹å³ä¿å­˜ä¸€æ¬¡ï¼Œç¡®ä¿ Rust ç«¯æœ‰æ¨¡å‹å¯ç”¨
    save_model(model)

    # 2. ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # 3. æ•°æ®åº“è¿æ¥å’Œç¼“å†²åŒº
    db = get_mongo_db()
    collection = db[COLLECTION_NAME]
    buffer = DataBuffer(MAX_SAMPLE_BUFFER_SIZE)
    
    print(f"[Training] ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    try:
        last_id = None
        total_batches_trained = 0
        total_loss_sum = 0.0
        total_policy_loss_sum = 0.0
        total_value_loss_sum = 0.0
        
        # --- åˆ†æ‰¹åŠ è½½å’Œè®­ç»ƒ ---
        while True:
            # æ¸…ç©ºç¼“å†²åŒºï¼Œå‡†å¤‡åŠ è½½æ–°ä¸€æ‰¹æ•°æ®
            buffer.boards.clear()
            buffer.scalars.clear()
            buffer.probs.clear()
            buffer.values.clear()
            buffer.masks.clear()
            
            # ä»æ•°æ®åº“åŠ è½½ä¸€æ‰¹æ¸¸æˆ
            query = {"_id": {"$gt": last_id}} if last_id else {}
            cursor = collection.find(query).sort('_id', 1).limit(FETCH_LIMIT)
            new_docs = list(cursor)
            
            if not new_docs:
                break  # æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œè®­ç»ƒç»“æŸ
            
            # å°†æ¸¸æˆæ ·æœ¬åŠ è½½åˆ°ç¼“å†²åŒº
            count_new_samples = 0
            for doc in new_docs:
                if 'samples' in doc and doc['samples']:
                    buffer.add_samples(doc['samples'])
                    count_new_samples += len(doc['samples'])
            
            last_id = new_docs[-1]['_id']
            print(f"[Training] ğŸ“¥ åŠ è½½ {len(new_docs)} å±€æ¸¸æˆï¼Œ{count_new_samples} ä¸ªæ ·æœ¬")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®è®­ç»ƒ
            if len(buffer) < BATCH_SIZE:
                print(f"[Training] âš ï¸ æ ·æœ¬ä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡ï¼Œè·³è¿‡")
                continue
            
            # è®­ç»ƒè¿™æ‰¹æ•°æ®
            indices = list(range(len(buffer)))
            random.shuffle(indices)
            
            num_batches = len(indices) // BATCH_SIZE
            batch_total_l, batch_pol_l, batch_val_l = 0.0, 0.0, 0.0
            
            for step in range(num_batches):
                batch_indices = indices[step * BATCH_SIZE : (step + 1) * BATCH_SIZE]
                batch_data = buffer.get_batch(batch_indices)
                
                tl, pl, vl = train_step(model, optimizer, batch_data, DEVICE)
                
                batch_total_l += tl
                batch_pol_l += pl
                batch_val_l += vl
                total_batches_trained += 1
            
            # ç´¯è®¡æŸå¤±
            total_loss_sum += batch_total_l
            total_policy_loss_sum += batch_pol_l
            total_value_loss_sum += batch_val_l
            
            # è¾“å‡ºè¿™æ‰¹æ•°æ®çš„è®­ç»ƒç»Ÿè®¡
            if num_batches > 0:
                avg_l = batch_total_l / num_batches
                avg_p = batch_pol_l / num_batches
                avg_v = batch_val_l / num_batches
                print(f"[Training] è®­ç»ƒ {num_batches} æ‰¹æ¬¡ - Loss: {avg_l:.4f} (Pol: {avg_p:.4f}, Val: {avg_v:.4f})")
        
        # è¾“å‡ºæ€»ä½“è®­ç»ƒç»Ÿè®¡
        if total_batches_trained > 0:
            overall_avg_loss = total_loss_sum / total_batches_trained
            overall_avg_pol = total_policy_loss_sum / total_batches_trained
            overall_avg_val = total_value_loss_sum / total_batches_trained
            print(f"\n[Training] âœ… è®­ç»ƒå®Œæˆï¼æ€»è®¡ {total_batches_trained} æ‰¹æ¬¡")
            print(f"[Training] å¹³å‡ Loss: {overall_avg_loss:.4f} (Pol: {overall_avg_pol:.4f}, Val: {overall_avg_val:.4f})")
        else:
            print("[Training] âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œè®­ç»ƒ")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        save_model(model)
        print("[Training] ğŸ‰ æ¨¡å‹å·²ä¿å­˜")

    except KeyboardInterrupt:
        print("[Training] Stopping...")
        save_model(model)
    except Exception as e:
        print(f"[Training] âŒ Error: {e}")
        save_model(model)

if __name__ == "__main__":
    main()